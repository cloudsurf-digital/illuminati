interval:   60
#logfile:    sauron.log
# Emitters are where you want to publish the data
#emitters:
#    # Use InfluxDB
#    InfluxDbPush:
#      host: my-influxdb.example.com
#      port: 8086
#      user: illuminati
#      password: s3cre3t
#      dbname: illuminati
#    # For example, we support CloudWatch
#    CloudWatch:
#        namespace: sauron
#        # If you'd like, specify additional dimensions for grouping
#        dimensions:
#            # If you'd like the instance ID to be automatically filled
#            # in, you can provide this key, without any value. Instead,
#            # to override it, you can instead provide a value.
#            InstanceId: 
#            project: sauron
#            name: dan
#            role: crawler
#        # Bake in actions, too
#        actions:
#            emailAdmin:
#                # Actions default to the email protocol
#                - {endpoint: dan@seomoz.org, protocol: email}
#                # - {endpoint: tech@seomoz.org}
#        # It's also nice to bake in your alarms
#        alarms:
#            serverDied:
#                # These are generic CloudWatch MetricAlarm options:
#                # http://docs.amazonwebservices.com/AmazonCloudWatch/latest/APIReference/API_MetricAlarm.html
#                description: The server process died!
#                metric: apache-count
#                statistic: Average
#                period: 60
#                threshold: 0
#                comparison: <=
#                evaluation_periods: 1
#                # Here, list the actions to take when this alarm is in state:
#                # - alarm
#                # - insufficient data
#                # - ok
#                alarm:
#                    - emailAdmin
#                insufficient_data:
#                    - emailAdmin
#                # ok:
#                #   - emailAdmin
#                # dimensions:
#                #   role: crawler
#    
#    # We also support Twitter
#    # This is kind of ugly, but... it's how it's done
#    Twitter:
#        consumer_key: yourAppsConsumerKey
#        consumer_secret: yourAppsSecretKey
#        access_token: yourAccountsAccessTokenForYourApp
#        access_secret: yourAccountsSecretTokenForYourApp
#    
## Metrics are what you want to collect
#metrics:
#    # Every metric has a name, which is the
#    # YAML key. Then, they must also specify the
#    # module that collects that data, and then
#    # any arguments that metric supports:
#    #
#    # name:
#    #   module: moduleName
#    #   modArg1: value1
#    #   modArg2: value2
#    #   ...
#    
#    # This logs the percent usage and the number of
#    # free inodes on the volume mounted at /
#    root:
#        module: DiskMetric
#        path: /
#        keys: 
#            - percent
#            - inodes
#    
#    # This looks at the apache access log, and counts
#    # the number of lines that match [POST], [GET]
#    apache:
#        module: LogMetric
#        path: /var/log/apache2/access.log
#        post: \[POST\]
#        get: \[GET\]
#    
#    # This checks the health of the mysql instance
#    mysql:
#        module: MySQLMetric
#        host: localhost
#        user: user
#        passwd: somepassword
#    
#    # This just reports the latency to the provided url
#    google:
#        module: PingMetric
#        url: http://google.com
#    
#    # Monitors a process named python, instantiated by
#    # 'dlecocq', in a provided path, run with args that
#    # match the listed regex
#    python:
#        module: ProcMetric
#        args: sauron
#        cwd: /some/sort
#        user: dlecocq
#    
#    # Monitors sphinx health
#    sphinx:
#        module: SphinxMetric
#
#    # Runs a command to get a metric
#    users:
#        module: ShellMetric
#        cmd: who | wc -l
#        units: Count
#    
#    # General details on the system
#    localhost:
#        module: SystemMetric
#    
#    # Some S3 status
#    buckets:
#        module: S3BucketMetric
#        bucket: someBucket
#        aws_id: yourAWSAccessKeyId
#        aws_secret: yourAWSAccessSecretKey
#        keys:
#            - oldest
#            - count
#    
#    # Redis monitoring
#    redis:
#        module: RedisMetric
#        host: my-redis-server
#        password: mySecretPassword
#        port: 6379
#        # The keys we want to save from Redis' info command
#        info:
#            - uptime_in_seconds
#            - used_cpu_sys
#            - used_cpu_user
#            - used_memory
#            - mem_fragmentation_ratio
#            - changes_since_last_save
#            - total_commands_processed
#        # The keys we should get, and interpret as a number
#        get:
#            - stats-jobs-finished
#            - stats-jobs-failed
#        # The keys we should lookup and get the length of
#        llen:
#            - queue-work
#            - queue-finished
#            - queue-failed
#        # The keys we should lookup and get the hash length of
#        hlen:
#            - myhash
#        # 'Nuff said
#        hget:
#            stats: wins
#            stats: losses
#        # Again, 'nuff said
#        scard:
#            - myset
